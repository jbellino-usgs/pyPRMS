

from __future__ import (absolute_import, division, print_function)
from future.utils import iteritems

import re
import numpy as np
import pandas as pd

from pyPRMS.prms_helpers import dparse


class Streamflow(object):
    # Library to work with PRMS streamflow data files that were generated by
    #   class gov.usgs.trinli.ft.point.writer.PrmsWriter

    def __init__(self, filename, missing=-999.0, verbose=False, include_metadata=True):
        # 1) open file
        # 2) get the metaheaders
        #    get the number of header lines
        # 3) get the station list
        # 4)

        self.__missing = missing
        self.filename = filename
        self.__verbose = verbose
        self.__include_metadata = include_metadata

        self.__timecols = 6  # number columns for time in the file
        self.__headercount = None
        self.__metaheader = None
        self.__types = None
        self.__units = []
        self.__stations = None
        self.__stationIndex = {}  # Lookup of station id to header info
        self.__rawdata = None
        self.__selectedStations = None
        self.__isloaded = False

        self.load_file(self.filename)

    @property
    def headercount(self):
        # Description: Returns the line number where the data begins in the given
        #              filename.
        #        Date: 2013-07-01
        if not self.__isloaded:
            self.load_file(self.filename)
        return self.__headercount

    @property
    def metaheader(self):
        # Description: Reads the "meta" header from the prms
        #        Date: 2013-06-25

        if not self.__isloaded:
            self.load_file(self.filename)
        return self.__metaheader

    @property
    def stations(self):
        # Description: Get the list of stations in the prms file.
        #        Note: Modified to return a list of all fields for each station

        # The order of the 'Type' field dictates the gross order of the following
        # data section.  For a given 'Type' the order of the 'ID' (stations) dictates
        # order of the data.

        # Get the meta-headers for the file
        if not self.__isloaded:
            self.load_file(self.filename)
        return self.__stations

    @property
    def timecolcnt(self):
        return self.__timecols

    @property
    def types(self):
        if not self.__isloaded:
            self.load_file(self.filename)
        return self.__types

    @property
    def data(self):
        if not self.__isloaded:
            self.load_file(self.filename)

        if self.__selectedStations is None:
            return self.__rawdata
        else:
            return self.__rawdata.ix[:, self.__selectedStations]

    @property
    def date_range(self):
        if not self.__isloaded:
            self.load_file(self.filename)

        # Return the first and last available date for valid streamflow data
        # 2015-05-19: This currently assumes it is returning a single streamgage
        tmpdf = self.data.dropna(axis=0, how='any')
        first_date = tmpdf[tmpdf.notnull()].index.min()
        last_date = tmpdf[tmpdf.notnull()].index.max()

        return first_date, last_date

    @property
    def numdays(self):
        """The period of record in days"""
        if not self.__isloaded:
            self.load_file(self.filename)
        return self.data.shape[0]

    @property
    def size(self):
        """Returns the number of stations included in the file"""
        return len(self.stations)

    @property
    def timedata(self):
        """Returns an array of time information"""
        # FIXME: This needs to be updated (2015-02-03)
        return self.data[:, 0:self.timecolcnt].astype(int)

    @property
    def units(self):
        return self.__units

    def load_file(self, filename):

        self.__selectedStations = None  # Clear out any selected stations
        self.__metaheader = []  # Holds the column names
        self.__types = {}  # dictionary of 'Type' field in order of occurrence
        self.__units = []  # list of units in file
        self.__stations = []  # list of gage stations
        self.__stationIndex = {}  # Lookup of station id to header info

        # headerNext = False
        # stationNext = False

        infile = open(filename, 'r')
        rawdata = infile.read().splitlines()
        infile.close()

        it = iter(rawdata)

        self.__headercount = 0
        # We assume if 'ID' and 'Type' header names exist then we have a valid
        # meta-header.
        for line in it:
            self.__headercount += 1

            # print line[0:10]
            # Skip through lines until we hit the following
            if line[0:10] == '// Station':
                # Read the next line in - this is the header info
                self.__headercount += 1
                self.__metaheader = re.findall(r"[\w]+", next(it))
                break

        cnt = 0
        order = 0  # defines the order of the data types in the dataset
        # curr_fcnt = 0
        st = 0

        # print '-'*10,'metaheader','-'*10
        # print self.__metaheader

        # Read the station IDs and optional additional metadata
        for line in it:
            self.__headercount += 1
            if line[0:10] == '//////////':
                break

            # Read in station information
            # Include question mark in regex below as a valid character since the obs
            # file uses it for missing data.
            words = re.findall(r"[\w.-]+|[?]", line)  # Break the row up
            curr_fcnt = len(words)

            # Check that number of fields remains constant
            if curr_fcnt != len(self.__metaheader):
                if self.__verbose:
                    print("WARNING: number of header fields changed from %d to %d" %
                          (len(self.__metaheader), curr_fcnt)),
                    print("\t", words)
                    # exit()

            try:
                if words[self.__metaheader.index('Type')] not in self.__types:
                    # Add unique station types (e.g. precip, runoff) if a 'Type' field exists in the metaheader
                    st = cnt  # last cnt becomes the starting column of the next type
                    order += 1

                # Information stored in __types array:
                # 1) Order that type was added in
                # 2) Starting index for data section
                # 3) Ending index for data section
                self.__types[words[self.__metaheader.index('Type')]] = [order, st, cnt]
            except ValueError:
                if self.__verbose:
                    print('No "Type" metadata; skipping.')

            self.__stations.append(words)
            self.__stationIndex[words[0]] = cnt
            cnt += 1
        
        # print self.__types

        # Now read in units and add to each type
        # print '-'*10,'UNITS','-'*10
        # print line
        unittmp = next(it).split(':')[1].split(',')
        for xx in unittmp:
            unit_pair = xx.split('=')
            # print 'unit_pair:', unit_pair[0].strip(), '/', unit_pair[1].strip()
            # self.__units[unit_pair[0].strip()] = unit_pair[1].strip()
            self.__units.append([unit_pair[0].strip(), unit_pair[1].strip()])

        # print self.__units

        # Skip to the data section
        for line in it:
            self.__headercount += 1
            if line[0:10] == '##########':
                self.__headercount += 1  # plus one for good measure
                break

        # print 'headercount:', self.__headercount
        # Data section

        # The first 6 columns are [year month day hour minute seconds]
        thecols = ['year', 'month', 'day', 'hour', 'min', 'sec']

        # Add the remaining columns to the list
        for xx in self.__stations:
            thecols.append(xx[0])

        # print 'thecols:', thecols

        # Use pandas to read the data in from the remainder of the file
        # We use a custom date parser to convert the date information to a datetime
        self.__rawdata = pd.read_csv(self.filename, skiprows=self.__headercount, sep=r"\s+",
                                     header=None, names=thecols,
                                     parse_dates={'thedate': ['year', 'month', 'day', 'hour', 'min', 'sec']},
                                     date_parser=dparse, index_col='thedate')

        # Convert the missing data (-999.0) to NaNs
        self.__rawdata.replace(to_replace=self.__missing, value=np.nan, inplace=True)

        # print self.__rawdata.head()
        self.__isloaded = True

    def get_data_by_type(self, thetype):
        """Returns data selected type (e.g. runoff)"""

        if thetype in self.__types:
            # print "Selected type '%s':" % (thetype), self.__types[thetype]
            st = self.__types[thetype][1]
            en = self.__types[thetype][2]
            # print "From %d to %d" % (st, en)
            b = self.data.iloc[:, st:en + 1]

            return b
        else:
            print("not found")

    def get_stations_by_type(self, thetype):
        """Returns station IDs for a given type (e.g. runoff)"""

        if thetype in self.__types:
            # print "Selected type '%s':" % (thetype), self.__types[thetype]
            st = self.__types[thetype][1]
            en = self.__types[thetype][2]
            # print "From %d to %d" % (st, en)
            b = self.stations[st:en + 1]

            return b
        else:
            print("not found")

    def select_by_station(self, streamgages):
        """Selects one or more streamgages from the dataset"""
        # The routine writeSelected() will write selected streamgages and data
        # to a new PRMS streamflow file.
        # Use clearSelectedStations() to clear any current selection.
        if isinstance(streamgages, list):
            self.__selectedStations = streamgages
        else:
            self.__selectedStations = [streamgages]

    def clear_selected_stations(self):
        """Clears any selected streamgages"""
        self.__selectedStations = None

    def write_selected_stations(self, filename):
        """Writes station observations to a new file"""
        # Either writes out all station observations or, if stations are selected,
        # then a subset of station observations.

        # Sample header format

        # $Id:$
        # ////////////////////////////////////////////////////////////
        # // Station metadata (listed in the same order as the data):
        # // ID    Type Latitude Longitude Elevation
        # // <station info>
        # ////////////////////////////////////////////////////////////
        # // Unit: runoff = ft3 per sec, elevation = feet
        # ////////////////////////////////////////////////////////////
        # runoff <number of stations for each type>
        # ################################################################################

        top_line = '$Id:$\n'
        section_sep = '////////////////////////////////////////////////////////////\n'
        meta_header_1 = '// Station metadata (listed in the same order as the data):\n'
        # metaHeader2 = '// ID    Type Latitude Longitude Elevation'
        meta_header_2 = '// %s\n' % ' '.join(self.metaheader)
        data_section = '################################################################################\n'

        # ----------------------------------
        # Get the station information for each selected station
        type_count = {}  # Counts the number of stations for each type of data (e.g. 'runoff')
        stninfo = ''
        if self.__selectedStations is None:
            for xx in self.__stations:
                if xx[1] not in type_count:
                    # index 1 should be the type field
                    type_count[xx[1]] = 0
                type_count[xx[1]] += 1

                stninfo += '// %s\n' % ' '.join(xx)
        else:
            for xx in self.__selectedStations:
                cstn = self.__stations[self.__stationIndex[xx]]

                if cstn[1] not in type_count:
                    # index 1 should be the type field
                    type_count[cstn[1]] = 0

                type_count[cstn[1]] += 1

                stninfo += '// %s\n' % ' '.join(cstn)
        # stninfo = stninfo.rstrip('\n')

        # ----------------------------------
        # Get the units information
        unit_line = '// Unit:'
        for uu in self.__units:
            unit_line += ' %s,' % ' = '.join(uu)
        unit_line = '%s\n' % unit_line.rstrip(',')

        # ----------------------------------
        # Create the list of types of data that are being included
        tmpl = []

        # Create list of types in the correct order
        for (kk, vv) in iteritems(self.__types):
            if kk in type_count:
                tmpl.insert(vv[0], [kk, type_count[kk]])

        type_line = ''
        for tt in tmpl:
            type_line += '%s %d\n' % (tt[0], tt[1])
        # typeLine = typeLine.rstrip('\n')

        # Write out the header to the new file
        outfile = open(filename, 'w')
        outfile.write(top_line)
        outfile.write(section_sep)
        outfile.write(meta_header_1)
        outfile.write(meta_header_2)
        outfile.write(stninfo)
        outfile.write(section_sep)
        outfile.write(unit_line)
        outfile.write(section_sep)
        outfile.write(type_line)
        outfile.write(data_section)

        # Write out the data to the new file
        # Using quoting=csv.QUOTE_NONE results in an error when using a customized  date_format
        # A kludgy work around is to write with quoting and then re-open the file
        # and write it back out, stripping the quote characters.
        self.data.to_csv(outfile, index=True, header=False, date_format='%Y %m %d %H %M %S', sep=' ')
        outfile.close()

        old = open(filename, 'r').read()
        new = re.sub('["]', '', old)
        open(filename, 'w').write(new)

        # def getRecurrenceInterval(self, thetype):
        #     """Returns the recurrence intervals for each station"""
        #
        #     # Copy the subset of data
        #     xx = self.seldata(thetype)
        #
        #     ri = np.zeros(xx.shape)
        #     ri[:,:] = -1.
        #
        #     # for each station we need to compute the RI for non-zero values
        #     for ss in range(0,xx.shape[1]):
        #         tmp = xx[:,ss]              # copy values for current station
        #
        #         # Get array of indices that would result in a sorted array
        #         sorted_ind = np.argsort(tmp)
        #         #print "sorted_ind.shape:", sorted_ind.shape
        #
        #         numobs = tmp[(tmp > 0.0),].shape[0]  # Number of observations > 0.
        #         nyr = float(numobs / 365)     # Number of years of non-zero observations
        #
        #         nz_cnt = 0  # non-zero value counter
        #         for si in sorted_ind:
        #             if tmp[si] > 0.:
        #                 nz_cnt += 1
        #                 rank = numobs - nz_cnt + 1
        #                 ri[si,ss] = (nyr + 1.) / float(rank)
        #                 #print "%s: [%d]: %d %d %0.3f %0.3f" % (ss, si,  numobs, rank, tmp[si], ri[si,ss])
        #
        #     return ri
# ***** END of class streamflow()
